{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLTOT13PyXVU"
      },
      "source": [
        "# **Reinforcement learning tutorial**\n",
        "In today's tutorial you will learn how apply Q-learning algorithm and deep Q-networks on simulated environments.\n",
        "\n",
        "We will use [**TensorFlow**](https://ekababisong.org/gcp-ml-seminar/tensorflow/) framework and [**Keras**](https://keras.io/) open-source library to rapidly prototype deep neural networks."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ywHWGctblX3r"
      },
      "source": [
        "# **Preliminary operations**\n",
        "The following code installs:\n",
        "- some supplementary modules useful to visualization purposes;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDXhM5SNlONs"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gym==0.19.0\n",
        "!pip install pyglet==1.5.27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ogh0deFhkSd"
      },
      "source": [
        "# **OpenAI Gym**\n",
        "[**Gym**](https://www.gymlibrary.ml/) is a toolkit, developed by the [**OpenAI**](https://openai.com/) company, providing an easy to set up, general-intelligence benchmark with a wide variety of different environments for developing and comparing reinforcement learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzHxqnm30C9F"
      },
      "source": [
        "# **Useful modules import**\n",
        "First of all, it is necessary to import useful modules used during the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_LtjnUptc_J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "import statistics\n",
        "import cv2\n",
        "import uuid\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from IPython.display import clear_output\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from collections import deque\n",
        "from pyvirtualdisplay import Display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGP7KdAG1x_H"
      },
      "source": [
        "# **Utility functions**\n",
        "Execute the following code to define some utility functions used in the tutorial:\n",
        "- **print_taxi_step** draws a single frame of the taxi environment with the corresponding information;\n",
        "- **print_taxi_single_episode** visualizes an entire episode of the taxi environment;\n",
        "- **plot_training_rewards** draws in a graph the total reward trend and its moving average reached during the different episodes of the training process;\n",
        "- **plot_stacked_frames** shows a stack of frames;\n",
        "- **create_mp4_video_from_frames** creates an MP4 video file from a list of frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZlpBsy_10fD"
      },
      "outputs": [],
      "source": [
        "def print_taxi_step(frame, episode, step, state, action, reward, current_total_reward):\n",
        "  print(frame)\n",
        "  if episode != None:\n",
        "    print('Episode: ', episode)\n",
        "  if step != None:\n",
        "    print('Step: ', step)\n",
        "  print('State: ', state)\n",
        "  print('Action: ', action)\n",
        "  print('Reward: ', reward)\n",
        "  print('Total reward: ', current_total_reward)\n",
        "\n",
        "def print_taxi_single_episode(frames, max_steps = 50, seconds_to_sleep = .1, episode = None):\n",
        "  for i, frame in enumerate(frames):\n",
        "    if i >= max_steps:\n",
        "      break\n",
        "\n",
        "    clear_output(wait = True)\n",
        "    print_taxi_step(frame['frame'], episode, i, frame['state'], frame['action'], frame['reward'], frame['total_reward'])\n",
        "    time.sleep(seconds_to_sleep)\n",
        "\n",
        "def plot_training_rewards(rewards, moving_avg_window_size = None):\n",
        "  if moving_avg_window_size is not None:\n",
        "    moving_avg_total_reward = []\n",
        "    for i in range(len(rewards)):\n",
        "      window = rewards[max(0, i - moving_avg_window_size + 1) : i + 1]\n",
        "      window_avg = statistics.mean(window)\n",
        "      moving_avg_total_reward.append(window_avg)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize = (15, 5))\n",
        "\n",
        "  ax.plot(range(len(rewards)), rewards,label = 'Total reward', color = 'orange')\n",
        "  ax.set_xlabel('Episodes')\n",
        "  if moving_avg_window_size is not None:\n",
        "    ax.plot(range(len(moving_avg_total_reward)), moving_avg_total_reward, label = 'Total reward moving average')\n",
        "    ax.legend(loc = 'upper left')\n",
        "\n",
        "def plot_stacked_frames(stacked_frames):\n",
        "  _, axs = plt.subplots(1, stacked_frames.shape[2], figsize = (15, 5))\n",
        "  for i in range(stacked_frames.shape[2]):\n",
        "    axs[i].axis('off')\n",
        "    axs[i].imshow(stacked_frames[:, :, i],cmap = 'gray')\n",
        "\n",
        "def create_mp4_video_from_frames(frames,fps):\n",
        "  temp_video_path = 'tempfile.mp4'\n",
        "  compressed_path = '{}.mp4'.format(str(uuid.uuid4()))\n",
        "  \n",
        "  size=(frames[0].shape[1], frames[0].shape[0])\n",
        "  out = cv2.VideoWriter(temp_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, size)\n",
        "  \n",
        "  for i in range(len(frames)):\n",
        "      out.write(frames[i][..., ::-1].copy())  #rgb[...,::-1].copy()\n",
        "  out.release()\n",
        "\n",
        "  os.system(f\"ffmpeg -i {temp_video_path} -vcodec libx264 {compressed_path}\")\n",
        "\n",
        "  os.remove(temp_video_path)\n",
        "\n",
        "  return compressed_path"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Useful terms**\n",
        "\n",
        "- **Agent**: an entity that can perceive/explore the environment and act upon it.\n",
        "\n",
        "- **Environment**: where the agent learns and decides what actions to perform.\n",
        "\n",
        "- **Action**: actions are the moves taken by an agent within the environment.\n",
        "\n",
        "- **State**: state is a situation returned by the environment after each action taken by the\n",
        "agent.\n",
        "\n",
        "- **Reward**: a scalar feedback returned to the agent from the environment when it performs specific actions.\n",
        "\n",
        "- **Policy**: policy is a strategy applied by the agent for the next action based on the current state. It defines the agent behavior at a given time by mapping state to action.\n",
        "\n",
        "<h1><center><img src=https://raw.githubusercontent.com/nderus/Generative-models-Lesson/main/images/pacman.png width=\"400\"></center></h1>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Reinforcement Learning**\n",
        "- Reinforcement learning (RL) is a subfield of machine learning that deals with the task of training an agent to make sequential decisions based on the rewards or penalties it receives for each action taken.\n",
        "\n",
        "- In RL, an agent interacts with an environment and learns to take actions that maximize a cumulative reward signal over time.\n",
        "\n",
        "- RL has gained a lot of attention in recent years due to its success in various applications, such as game playing, robotics, and medical decision-making.\n",
        "\n",
        "- The core idea behind RL is to use trial-and-error learning to discover the optimal strategy for the agent. The agent receives feedback in the form of rewards or penalties for each action taken, and based on this feedback, it learns to adjust its behavior to maximize the overall reward. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Q-learning**\n",
        "- Q-learning is a popular reinforcement learning algorithm that is used to solve Markov decision processes (MDPs).\n",
        "\n",
        "- The algorithm works by iteratively updating estimates of the optimal action-value function Q(s,a), which represents the expected reward of taking action a in state s and following the optimal policy thereafter.\n",
        "\n",
        "- At each iteration, the algorithm selects an action based on an epsilon-greedy policy, which balances exploration and exploitation, and observes the reward and the next state. It then uses the observed information to update the estimate of Q(s,a) using the Bellman equation:\n",
        "\n",
        "$$Q(s,a) ← Q(s,a) + α [r + γ max_a' Q(s',a') - Q(s,a)]$$\n",
        "\n",
        "where α is the learning rate, r is the observed reward, γ is the discount factor, s' is the next state, and a' is the action that maximizes the expected reward from s'.\n",
        "\n",
        "- The algorithm repeats this process until the estimates of Q converge to their optimal values, which is guaranteed to happen under certain assumptions. Once the optimal Q-function has been learned, the optimal policy can be derived by selecting the action with the highest Q-value in each state.\n",
        "\n",
        "- Q-learning is a powerful and versatile algorithm that has been applied to a wide range of problems, from playing games to controlling robots. However, it can be slow to converge in large and complex environments, and its performance can be sensitive to the choice of hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYjwygcU0RMd"
      },
      "source": [
        "# **Q-learning with self-driving taxi**\n",
        "In this section, the Q-Learning algorithm is applied to a self-driving taxi that will need to learn how to transport its passengers to the desired destination. \n",
        "\n",
        "To this purpose, the [**Taxi**](https://www.gymlibrary.ml/environments/toy_text/taxi/) environment provided by Gym will be used:\n",
        "\n",
        "\"*There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.*\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IHsAL3OJ02aE"
      },
      "source": [
        "## **Taxi environment**\n",
        "The environment is represented by a training area for the self-driving taxi where to learn transport people in four different locations (R, G, Y, B):\n",
        "\n",
        "<img src=https://raw.githubusercontent.com/nderus/Generative-models-Lesson/main/images/Reinforcement_Learning_Taxi_Env.png width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvPFemVmjyFt"
      },
      "source": [
        "### **Creation**\n",
        "The following code creates the Taxi environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4hCHiFouptT"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Taxi-v3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbwXGzkCjMel"
      },
      "source": [
        "### **Visualization**\n",
        "To visualize the environment, the [**render**](https://www.gymlibrary.ml/content/api/#rendering) method can be used:\n",
        "- the filled square represents the taxi, which is yellow without a passenger and green with a passenger;\n",
        "- the pipe (\"|\") represents a wall which the taxi cannot cross;\n",
        "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3qxaLHOjMpl"
      },
      "outputs": [],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdKE599ophLI"
      },
      "source": [
        "### **State space**\n",
        "Assuming self-driving taxi is the only vehicle in this parking lot, the parking lot can be divided into a $5\\times 5$ grid, obtaining 25 possible taxi locations. Four of them are locations where the taxi can pick up and drop off a passenger: R, G, Y, B or $[(0,0), (0,4), (4,0), (4,3)]$ in $(row, col)$ coordinates.\n",
        "\n",
        "Moreover, there are 4 possible destinations and 5 (R, G, Y, B or inside the taxi) passenger locations.\n",
        "\n",
        "The taxi environment has $5\\times 5\\times 5\\times 4=500$ total possible states.\n",
        "\n",
        "Every Gym environment comes with an [**observation_space**](https://www.gymlibrary.ml/content/api/#attributes) attribute describing the format of valid observations (or states). If the space is composed by a fixed number of discrete values, an instance of the [**Discrete**](https://www.gymlibrary.ml/content/api/#spaces) space class is returned and the **n** property can be used to get the cardinality of the space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hjWXtTM-AAM"
      },
      "outputs": [],
      "source": [
        "print('There are {} possible states'.format(env.observation_space.n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkJakNGIptmG"
      },
      "source": [
        "### **Action space**\n",
        "The action space is composed by six possible actions:\n",
        "- south;\n",
        "- north;\n",
        "- east;\n",
        "- west;\n",
        "- pickup;\n",
        "- dropoff.\n",
        "\n",
        "Every Gym environment comes with an [**action_space**](https://www.gymlibrary.ml/content/api/#additional-environment-api) attribute describing the format of valid actions. As for the state space, the action space is discrete and the number of possible actions can be retrieved by the **n** property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoUgl4f4-Lcc"
      },
      "outputs": [],
      "source": [
        "print('There are {} possible actions'.format(env.action_space.n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8d2IlWCgw5F"
      },
      "source": [
        "### **Rewards**\n",
        "The rewards are:\n",
        "- -1 for each step;\n",
        "- -1 for every wall hit (the taxi will not move anywhere);\n",
        "- +20 for successfully deliver the passenger;\n",
        "- -10 for illegal actions (pick up or put down the passenger in the wrong location)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IrcYWOR4vx-w"
      },
      "source": [
        "## **Exercise 1: Q-learning algorithm**\n",
        "Implement the **q_learning** function given:\n",
        "- the environment (*env*);\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the learning rate $\\alpha$ (alpha);\n",
        "- the discount factor $\\gamma$ (gamma).\n",
        "\n",
        "It returns the learned Q-table (*Q*) and a **List** (*learning_history*) containing useful information to visualize the agent progresses during the learning process.\n",
        "\n",
        "The following image shows the pseudo-code of the *Q-learning* algorithm.\n",
        "\n",
        "<img src=https://raw.githubusercontent.com/nderus/Generative-models-Lesson/main/images/Q_learning_algorithm.png width=\"800\">\n",
        "\n",
        "The following parts need to be implemented:\n",
        "1. initialize *Q* as a Numpy zero matrix with as many rows as the states and as many columns as the actions. The [**zeros**](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html) function provided by the Numpy module can be used;\n",
        "2. initialize $\\epsilon$ equal to *max_epsilon*;\n",
        "3. call the **reset** method provided by the Gym environment interface to reset the environment and get the random initial state ($s_0$);\n",
        "4. decide whether to pick a random action or to exploit the already computed Q-values. To this purpose, the **uniform** function provided by the [**random**](https://docs.python.org/3/library/random.html) module can be used to generate a random number in the range $[0;1]$ to be compared against $\\epsilon$. If a random action needs to be selected from the set of all possible actions, the **action_space.sample** method provided by the Gym environment can be exploited. Otherwise, use the [**argmax**](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) function provided by Numpy to select the optimal action for state $s_t$;\n",
        "5. call the **step** method provided by the Gym environment interface to\n",
        "execute the action $a_t$ selected at the previous point. It returns the new observation (or state) $s_{t+1}$, the reward $r_{t+1}$ achieved by action $a_t$, a boolean flag indicating if the episode has terminated (*True*) or not (*False*) and other additional information;\n",
        "6. update $Q(s_t,a_t)$ using the *Bellman* equation. The Numpy [**amax**](https://numpy.org/doc/stable/reference/generated/numpy.amax.html) function can be used to find the maximum value of $Q(s_{t+1},a)$;\n",
        "7. reduce $\\epsilon$. It is progressively reduced during the learning process because, while the agent learns more and more about the environment, less and less exploration will be needed;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDapFnFEMRF6"
      },
      "outputs": [],
      "source": [
        "def q_learning(env,episode_count,episode_max_steps,max_epsilon,min_epsilon,epsilon_decay,alpha,gamma):\n",
        "  # 1. initialize Q as a zero matrix with as many rows as the states and as many columns as the actions \n",
        "  Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "  learning_history = [] # for visualization purposes\n",
        "\n",
        "  # 2. inizialize epsilon equal to max_epsilon\n",
        "  epsilon=max_epsilon\n",
        "\n",
        "  for episode in range(episode_count):\n",
        "      # for visualization purposes\n",
        "      episode_history = [] \n",
        "      episode_total_reward=0\n",
        "      \n",
        "      # 3. reset the environment\n",
        "      state = env.reset()\n",
        "      \n",
        "      step_count=0  #t\n",
        "      done=False\n",
        "      while (step_count<episode_max_steps) and (not done):\n",
        "          # 4. decide whether to pick a random action or to exploit the already computed Q-values\n",
        "          if(random.uniform(0,1) <= epsilon):\n",
        "            action = env.action_space.sample()  #--> exploration\n",
        "          else:\n",
        "            action = np.argmax(Q[state])  #--> exploitation\n",
        "    \n",
        "          # 5. take the action and observe the outcome state and reward\n",
        "          new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "          # 6. update Q(s,a)\n",
        "          Q[state][action] = (1 - alpha) * Q[state][action] + alpha * (reward + gamma * np.amax(Q[new_state]))\n",
        "\n",
        "          # put the current frame into a list for visualization purposes\n",
        "          episode_total_reward+=reward\n",
        "          episode_history.append(\n",
        "                                  {\n",
        "                                  'frame': env.render(mode='ansi'),\n",
        "                                  'state': new_state,\n",
        "                                  'action': action,\n",
        "                                  'reward': reward,\n",
        "                                  'total_reward' : episode_total_reward,\n",
        "                                  }\n",
        "                                )\n",
        "\n",
        "          # update the current state\n",
        "          state = new_state\n",
        "\n",
        "          # increase the step count\n",
        "          step_count+=1\n",
        "\n",
        "      # 7. reduce epsilon\n",
        "      if (epsilon>min_epsilon):\n",
        "        epsilon=max(min_epsilon,epsilon-epsilon_decay)\n",
        "\n",
        "      # put all the episode frames into a list for visualization purposes\n",
        "      learning_history.append(episode_history)\n",
        "\n",
        "  # return Q-table and history\n",
        "  return Q, learning_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8z0zryiIvhy"
      },
      "source": [
        "## **Learning**\n",
        "Now we are ready to start the learning process by calling the **q_learning** function.\n",
        "\n",
        "The following parameters must be set before being passed to the function:\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the learning rate $\\alpha$ (alpha);\n",
        "- the discount factor $\\gamma$ (gamma)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOBFLWhvAoRR"
      },
      "outputs": [],
      "source": [
        "episode_count = 2000          # Total number of training episodes\n",
        "episode_max_steps = 200       # Maximum number of steps per episode\n",
        "\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.001           # Minimum exploration probability \n",
        "epsilon_decay = 0.01          # Decay for exploration probability\n",
        "\n",
        "alpha = 0.5                   # Learning rate\n",
        "gamma = 0.99                  # Discount factor\n",
        "\n",
        "start_time = time.time()\n",
        "Q,learning_history = q_learning(env, episode_count, episode_max_steps, max_epsilon, min_epsilon, epsilon_decay, alpha, gamma)\n",
        "print('Learning time: {:.1f}s'.format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oVrsaOPqsT2"
      },
      "source": [
        "### **Visualize the learning process**\n",
        "It is important to observe how performance changes over time during the learning process.\n",
        "\n",
        "The following code draws in a graph the number of steps and the total reward reached during the different episodes of the learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le8wb4UaI3OC"
      },
      "outputs": [],
      "source": [
        "steps=[]\n",
        "total_rewards=[]\n",
        "for episode in learning_history:\n",
        "  steps.append(len(episode))\n",
        "  total_rewards.append(episode[-1]['total_reward'])\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(15, 5))\n",
        "\n",
        "ax1.plot(range(len(steps)), steps, color = 'orange')\n",
        "ax1.tick_params(axis = 'y', labelcolor = 'orange')\n",
        "ax1.set_xlabel('Episodes')\n",
        "ax1.set_ylabel('Steps', color = 'orange')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(range(len(total_rewards)), total_rewards, color = '#1f77b4')\n",
        "ax2.tick_params(axis = 'y', labelcolor = '#1f77b4')\n",
        "ax2.set_ylabel('Total reward', color = '#1f77b4')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1arMQr_huEJF"
      },
      "source": [
        "To better highlight how the self-driving taxi improves during the learning process, it is useful to visualize single episodes using the **print_taxi_single_episode** function defined above. The following code shows the first episode of the learning process (only the first 25 steps per episode are displayed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmPDvIaG60QW"
      },
      "outputs": [],
      "source": [
        "print_taxi_single_episode(learning_history[0], 25, 0.5, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5WKpR2MuboM"
      },
      "source": [
        "While initially the taxi randomly moves exploring the environment, at the end of the learning process the agent is perfectly capable to pickup a passenger and to transport him to the desired destination. The following code shows the last episode of the learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuzdpjtFvxpm"
      },
      "outputs": [],
      "source": [
        "print_taxi_single_episode(learning_history[-1], 25, 0.5, len(learning_history) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGsTYVMq2q2i"
      },
      "source": [
        "## **Performance evaluation**\n",
        "Now it is time to evaluate the performance of the self-driving taxi agent. It can be evaluated according to the following metrics:\n",
        "- **average number of steps per episode**: the smaller the number, the shorter the path taken by the agent to reach the destination;\n",
        "- **average total reward per episode**: a higher average reward means that the agent reaches the destination as fast as possible with the least penalties;\n",
        "- **average number of penalties per episode**: it is computed as the average number of time per episode the agent executes an illegal action (pick up or put down the passenger in the wrong location). The smaller the number, the better the performance of our agent.\n",
        "\n",
        "By running the following code the evaluation metrics will be computed as the average of the results on several (*test_episode_count*) episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4AhzWZ_v8Uq"
      },
      "outputs": [],
      "source": [
        "test_episode_count = 1000     # Total number of test episodes\n",
        "\n",
        "sum_steps = 0\n",
        "sum_total_reward = 0\n",
        "sum_penalties = 0\n",
        "\n",
        "for episode in range(test_episode_count):\n",
        "    state = env.reset()\n",
        "    for step in range(episode_max_steps):\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(Q[state])\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        sum_steps += 1\n",
        "        sum_total_reward += reward\n",
        "        if reward == -10:\n",
        "          sum_penalties += 1\n",
        "\n",
        "        if done:\n",
        "          break\n",
        "        state = new_state\n",
        "\n",
        "print ('Average number of steps per episode: {:.1f}'.format(sum_steps / test_episode_count))\n",
        "print ('Average total reward per episode: {:.1f}'.format(sum_total_reward / test_episode_count))\n",
        "print ('Average number of penalties per episode: {:.1f}'.format(sum_penalties / test_episode_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaFRxFG9xJpq"
      },
      "source": [
        "## **Single episode visualization**\n",
        "The following code shows the self-driving in action on a single episode using the **print_taxi_step** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov9PI0hbtr4L"
      },
      "outputs": [],
      "source": [
        "episode_total_reward = 0\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "step = 0\n",
        "while not done:\n",
        "    action = np.argmax(Q[state])\n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    \n",
        "    episode_total_reward += reward\n",
        "    clear_output(wait = True)\n",
        "    print_taxi_step(env.render(mode = 'ansi'), None, step, new_state, action, reward, episode_total_reward)\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    state = new_state\n",
        "    step += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Deep Q-learning**\n",
        "Deep Q-Learning (DQL) is a variant of Q-Learning that uses a neural network as a function approximator to estimate the optimal action-value function, Q(s,a). In contrast to traditional Q-Learning, which stores Q-values in a table, DQL can handle high-dimensional state spaces, which makes it suitable for solving complex problems such as playing Atari games.\n",
        "\n",
        "In DQL, the neural network takes the state as input and outputs the estimated Q-values for all possible actions. The network is trained using a variant of the Q-Learning update rule, where the target Q-value is computed using a fixed target network to prevent oscillations and instability:\n",
        "\n",
        "y = r + γ max_a' Q'(s',a')\n",
        "\n",
        "where r is the reward, γ is the discount factor, s' is the next state, a' is the action that maximizes the Q-value in the next state, and Q' is the fixed target network. The loss function for training the network is then given by the mean squared error between the predicted and target Q-values:\n",
        "\n",
        "L = (y - Q(s,a))^2\n",
        "\n",
        "The network is trained using stochastic gradient descent to minimize the loss function. The weights of the target network are periodically updated to match the weights of the online network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFcs1UtzErww"
      },
      "source": [
        "# **Deep Q-network to solve the CartPole environment**\n",
        "In this section, a Deep Q-Network (DQN) model is trained to solve the [**CartPole**](https://www.gymlibrary.ml/environments/classic_control/cart_pole/) environment provided by Gym."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFfk_p18avWw"
      },
      "source": [
        "## **CartPole environment**\n",
        "The environment is represented by a pole attached to a cart which moves along a frictionless track. The objective is to prevent pole from falling over. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves far from the center."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvIwaKujbbZA"
      },
      "source": [
        "### **Creation**\n",
        "The following code creates the CartPole environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNj9XgWxbetA"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNu7MSFhbsSn"
      },
      "source": [
        "### **Visualization**\n",
        "To visualize the environment on *Colab*, [**PyVirtualDisplay**](https://pypi.org/project/PyVirtualDisplay/) is used to create a virtual display.\n",
        "\n",
        "The following code creates a virtual display where the environment can be rendered. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWUqfgoU4-dG"
      },
      "outputs": [],
      "source": [
        "display = Display()\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PExv4V66640p"
      },
      "source": [
        "To visualize the current state of the environment, the **render** method can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms8qQunW5CNB"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "prev_screen = env.render(mode = 'rgb_array')\n",
        "plt.axis('off')\n",
        "plt.imshow(prev_screen)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpCQj-ndQQcZ"
      },
      "source": [
        "### **State space**\n",
        "Differently from Taxi environment, the CartPole state space is represented by 4 continuous real values: \n",
        "- cart Position;\n",
        "- cart Velocity;\n",
        "- pole angle;\n",
        "- pole angular velocity.\n",
        "\n",
        "In this case, the **observation_space** attribute returns a [**Box**](https://www.gymlibrary.ml/content/api/#spaces) space class instance where valid states are in the form of *n*-dimensional arrays. The *shape* of the space can be obtained by the **shape** property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov0HK7xvQOTz"
      },
      "outputs": [],
      "source": [
        "print('State space shape: ', env.observation_space.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Si9Hj-sL82"
      },
      "source": [
        "### **Action space**\n",
        "The action space is composed by two possible actions:\n",
        "- push cart to the left;\n",
        "- push cart to the right.\n",
        "\n",
        "The following code uses the **n** property of the action space to print the number of valid actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KqXRJFHs2Xx"
      },
      "outputs": [],
      "source": [
        "print('There are {} possible actions'.format(env.action_space.n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNYRi3OAtPd5"
      },
      "source": [
        "### **Rewards**\n",
        "Reward is 1 for every step taken, including the termination step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec8i63BqG3C9"
      },
      "source": [
        "## **Model definition**\n",
        "The following function creates a simple neural network to be used as DQN given:\n",
        "- the number of input features (*input_count*);\n",
        "- the number of neurons for each hidden layer (*neuron_count_per_hidden_layer*);\n",
        "- the number of valid actions (*action_count*).\n",
        "\n",
        "In Keras, a sequential is a stack of layers where each layer has exactly one input and one output. It can be created by passing a list of layers to the  constructor [**keras.Sequential**](https://keras.io/guides/sequential_model/).\n",
        "\n",
        "[**Keras layers API**](https://keras.io/api/layers/) offers a wide range of built-in layers ready for use, including:\n",
        "- [**Input**](https://keras.io/api/layers/core_layers/input/) - the input of the model. Note that, you can also omit the **Input** layer. In that case the model doesn't have any weights until the first call to a training/evaluation method (since it is not yet built);\n",
        "- [**Dense**](https://keras.io/api/layers/core_layers/dense/) - a fully-connected layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwgvCwIBG79t"
      },
      "outputs": [],
      "source": [
        "def build_simple_dqn(input_count, neuron_count_per_hidden_layer, action_count):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape = input_count, name = 'Input'))\n",
        "\n",
        "    for n in neuron_count_per_hidden_layer:\n",
        "        model.add(layers.Dense(n, activation = 'relu'))\n",
        "\n",
        "    model.add(layers.Dense(action_count, name = 'Output'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njN42JeqJqb9"
      },
      "source": [
        "## **Action and target models creation**\n",
        "To solve the *moving target problem*, two models (*action* and *target*) are used during the DQN training process.\n",
        "\n",
        "The following code creates two identical models by calling the **build_simple_dqn** function defined above. To be identical, the two models need to share initial random weights: [**get_weights**](https://keras.io/api/models/model_saving_apis/#getweights-method) and [**set_weights**](https://keras.io/api/models/model_saving_apis/#setweights-method) methods are used to replace target weights to those of the action model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTT-qE2pJyTL"
      },
      "outputs": [],
      "source": [
        "neuron_count_per_hidden_layer=[64, 32]\n",
        "\n",
        "simple_dqn_action_model = build_simple_dqn(env.observation_space.shape, neuron_count_per_hidden_layer, env.action_space.n)\n",
        "simple_dqn_target_model = build_simple_dqn(env.observation_space.shape, neuron_count_per_hidden_layer, env.action_space.n)\n",
        "simple_dqn_target_model.set_weights(simple_dqn_action_model.get_weights())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bxm50-MNPG4"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed using the [**summary**](https://keras.io/api/models/model/#summary-method) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6bs9YZNNRfV"
      },
      "outputs": [],
      "source": [
        "simple_dqn_action_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCRi_phgNe3f"
      },
      "source": [
        "The summary is useful for simple models, but can be confusing for complex models.\n",
        "\n",
        "Function [**keras.utils.plot_model**](https://keras.io/api/utils/model_plotting_utils/) creates a plot of the neural network graph that can make more complex models easier to understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idq-ImEsNfpv"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(simple_dqn_action_model, show_shapes = True, show_layer_names = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlIggmE4OATx"
      },
      "source": [
        "## **Action model compilation**\n",
        "The compilation is the final step in configuring the model for training. \n",
        "\n",
        "The following code uses the [**compile**](https://keras.io/api/models/model_training_apis/#compile-method) method to compile the model.\n",
        "The important arguments are:\n",
        "- the optimization algorithm (*optimizer*);\n",
        "- the loss function (*loss*).\n",
        "\n",
        "The most common [optimization algorithms](https://keras.io/api/optimizers/#available-optimizers) and [loss functions](https://keras.io/api/losses/#available-losses) are already available in Keras. You can either pass them to **compile** as an instance or by the corresponding string identifier. In the latter case, the default parameters will be used.\n",
        "\n",
        "<u>Note that, only the action model is compiled because, during the training process, only its weights are updated using *gradient descent* algorithm. Target model weights are periodically replaced by those of the action model.</u>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kdoq1IBXOIIA"
      },
      "outputs": [],
      "source": [
        "simple_dqn_action_model.compile(optimizer = 'adam', loss = 'mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feB3RlCDUPIq"
      },
      "source": [
        "## **Experience replay**\n",
        "To remove correlations between consecutive transitions and make the DQN training more stable, the *experience replay* technique is used.\n",
        "\n",
        "The replay memory is implemented as a [**deque**](https://docs.python.org/3/library/collections.html#collections.deque) (Doubly Ended Queue) object providing an O(1) time complexity for append and pop operations from both the ends of the queue. Moreover, if the *maxlen* parameter is specified, the **deque** is bounded to the specified maximum length. Once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end.\n",
        "\n",
        "The DQN authors suggest to populate the replay memory before starting the learning process. For each step $t$, a random action is chosen and executed then the transition $<s_t,a_t,r_{t+1},s_{t+1}>$ is stored in the replay memory. The following function initializes the replay memory given:\n",
        "- the environment (*env*);\n",
        "- the replay memory (*replay_memory*);\n",
        "- the number of transitions to be stored in the replay memory (*replay_memory_init_size*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiCx7_GnUQBv"
      },
      "outputs": [],
      "source": [
        "def simple_dqn_replay_memory_init(env, replay_memory, replay_memory_init_size, episode_max_steps):\n",
        "    while True:\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        step_count = 0\n",
        "        while (step_count < episode_max_steps) and (not done):\n",
        "            action = env.action_space.sample()\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            replay_memory.append([state, action, reward, new_state, done])\n",
        "            state = new_state\n",
        "            step_count += 1\n",
        "            if len(replay_memory) >= replay_memory_init_size:\n",
        "                return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKCPCG1NbNgF"
      },
      "source": [
        "During training, when the action model needs to be updated, a mini-batch is randomly sampled from the replay memory and used instead of the most recent transition. The following function returns a mini-batch containing transitions randomly chosen from the replay memory given:\n",
        "- the replay memory (*replay_memory*);\n",
        "- the mini-batch size (*batch_size*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvNAIYg8bOgj"
      },
      "outputs": [],
      "source": [
        "def get_random_batch_from_replay_memory(replay_memory, batch_size):\n",
        "    minibatch_indices = np.random.choice(range(len(replay_memory)), size = batch_size)\n",
        "    minibatch = [replay_memory[i] for i in minibatch_indices]\n",
        "    \n",
        "    state_batch = np.array([sample[0] for sample in minibatch])\n",
        "    action_batch = np.array([sample[1] for sample in minibatch])\n",
        "    reward_batch = np.array([sample[2] for sample in minibatch])\n",
        "    new_state_batch = np.array([sample[3] for sample in minibatch])\n",
        "    done_batch = np.array([sample[4] for sample in minibatch])\n",
        "\n",
        "    return [state_batch, action_batch, reward_batch, new_state_batch, done_batch]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fo6cZDT9ticw"
      },
      "source": [
        "## **Exercise 2: DQN training algorithm**\n",
        "Implement the following function to train a DQN model given:\n",
        "- the environment (*env*);\n",
        "- the action model (*dqn_action_model*);\n",
        "- the target model (*dqn_target_model*);\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum number of transitions stored into the replay memory (*replay_memory_max_size*);\n",
        "- the number of transitions stored into the replay memory before starting the training process (*replay_memory_init_size*);\n",
        "- the mini-batch size (*batch_size*);\n",
        "- the number ($u$) of total steps executed ($T$) between successive updates of the action model weights (*step_per_update*);\n",
        "- the number ($c$) of total steps executed ($T$) between successive replaces of the target model weights with the weights of the action model (*step_per_update_target_model*); \n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the discount factor $\\gamma$ (gamma);\n",
        "- the number of consecutive episodes to be considered in the calculation of the total reward moving average (*moving_avg_window_size*);\n",
        "- the minimum value of the total reward moving average to consider the task solved (*moving_avg_stop_thr*). A value of *None* force to execute all *episode_count* episodes.\n",
        "\n",
        "It returns a **List** (*train_rewards*) containing the total rewards of all training episodes.\n",
        "\n",
        "The following image shows the pseudo-code of the DQN training algorithm.\n",
        "\n",
        "<img src=https://raw.githubusercontent.com/nderus/Generative-models-Lesson/main/images/DQN_training_algorithm.png width=\"800\">\n",
        "\n",
        "The following parts need to be implemented:\n",
        "1. call the **simple_dqn_replay_memory_init** function to populate the replay memory before starting the training process;\n",
        "2. initialize $\\epsilon$ equal to *max_epsilon*;\n",
        "3. call the **reset** method provided by the Gym environment interface to reset the environment and get the random initial state ($s_0$);\n",
        "4. decide whether to pick a random action or to exploit the already computed Q-values. To this purpose, the **uniform** function provided by the random module can be used to generate a random number in the range $[0;1]$ to be compared against $\\epsilon$. If a random action needs to be selected from the set of all possible actions, the **action_space.sample** method provided by the Gym environment can be exploited. Otherwise, use the [**predict**](https://keras.io/api/models/model_training_apis/#predict-method) method of the action model to obtain the Q-values of each available actions given the current state ($s_t$) and the  [**argmax**](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) function provided by Numpy to select the optimal one;\n",
        "5. call the **step** method provided by the Gym environment interface to\n",
        "execute the action $a_t$ selected at the previous point. It returns the new observation (or state) $s_{t+1}$, the reward $r_{t+1}$ achieved by action $a_t$, a boolean flag indicating if the episode has terminated (*True*) or not (*False*) and other additional information;\n",
        "6. call the **get_random_batch_from_replay_memory** function to get a mini-batch of randomly selected transitions from the replay memory;\n",
        "7. copy weights from action to target model using the **get_weights** and **set_weights** methods;\n",
        "8. reduce $\\epsilon$ at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G4gXI4_amyr"
      },
      "outputs": [],
      "source": [
        "def simple_dqn_training(env, dqn_action_model, dqn_target_model, episode_count, episode_max_steps,\n",
        "                        replay_memory_max_size, replay_memory_init_size, batch_size,\n",
        "                        step_per_update, step_per_update_target_model,\n",
        "                        max_epsilon, min_epsilon, epsilon_decay, gamma,\n",
        "                        moving_avg_window_size = 20, moving_avg_stop_thr = None):\n",
        "    # experience replay memory declaration\n",
        "    replay_memory = deque(maxlen = replay_memory_max_size)\n",
        "    \n",
        "    # 1. replay memory initial population\n",
        "    if replay_memory_init_size > 0:\n",
        "        print('Replay memory initial population')\n",
        "        simple_dqn_replay_memory_init(env, replay_memory, replay_memory_init_size, episode_max_steps)\n",
        "\n",
        "    # 2. inizialize epsilon equal to max_epsilon\n",
        "    epsilon = max_epsilon\n",
        "\n",
        "    train_rewards = []  # for visualization purposes\n",
        "\n",
        "    train_step_count = 0  #T\n",
        "    for n in range(episode_count): \n",
        "        # for visualization purposes\n",
        "        episode_start_time = time.time()    \n",
        "        episode_reward = 0\n",
        "        episode_epsilon = epsilon\n",
        "\n",
        "        # 3. reset the environment\n",
        "        state = env.reset()\n",
        "\n",
        "        step_count = 0  #t\n",
        "        done = False\n",
        "        while step_count < episode_max_steps and not done: \n",
        "            # 4. decide whether to pick a random action or to exploit the already computed Q-values\n",
        "            if random.uniform(0, 1) <= epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                q_values = dqn_action_model.predict(state[np.newaxis])\n",
        "                action = np.argmax(q_values)\n",
        "            \n",
        "            # 5. take the action and observe the outcome state and reward\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # store transition in the replay memory \n",
        "            replay_memory.append([state, action, reward, new_state, done])\n",
        "            \n",
        "            # update the current state\n",
        "            state = new_state\n",
        "                        \n",
        "            if train_step_count % step_per_update == 0 and len(replay_memory) >= batch_size:\n",
        "                # 6. get a random mini-batch from the replay memory\n",
        "                mini_batch = get_random_batch_from_replay_memory(replay_memory, batch_size)\n",
        "                \n",
        "                # update the action model weights calling the simple_dqn_update function (see below)\n",
        "                dqn_action_model = simple_dqn_update(dqn_action_model, dqn_target_model, mini_batch, gamma)\n",
        "                \n",
        "            if train_step_count % step_per_update_target_model == 0:\n",
        "                # 7. copy weights from action to target model\n",
        "                dqn_target_model.set_weights(dqn_action_model.get_weights())\n",
        "\n",
        "            # 8. reduce epsilon\n",
        "            if epsilon > min_epsilon:\n",
        "                epsilon = max(min_epsilon, epsilon - epsilon_decay)\n",
        "\n",
        "            # increase episode step count and total step count        \n",
        "            step_count += 1\n",
        "            train_step_count += 1\n",
        "\n",
        "            # add the current reward to the episode total reward\n",
        "            episode_reward += reward \n",
        "        \n",
        "        # put the episode total reward into a list for visualization purposes\n",
        "        train_rewards.append(episode_reward)\n",
        "\n",
        "        # for visualization purposes\n",
        "        episode_finish_time = time.time()\n",
        "        episode_elapsed_time = episode_finish_time - episode_start_time\n",
        "        episode_avg_step_time = episode_elapsed_time / step_count\n",
        "        moving_avg_reward = statistics.mean(train_rewards[- moving_avg_window_size:])\n",
        "        print(\"Episode: {} Steps: {}[{}] Epsilon: {:.3f} Time: {:.1f}s[{:.2f}s]  Total reward: {}[{:.1f}]\".format(n,\n",
        "                                                                                                                  step_count,\n",
        "                                                                                                                  train_step_count,\n",
        "                                                                                                                  episode_epsilon,\n",
        "                                                                                                                  episode_elapsed_time,\n",
        "                                                                                                                  episode_avg_step_time,\n",
        "                                                                                                                  episode_reward,\n",
        "                                                                                                                  moving_avg_reward))\n",
        "        \n",
        "        # condition to consider the task solved\n",
        "        if (moving_avg_stop_thr is not None) and moving_avg_reward>moving_avg_stop_thr:\n",
        "            break\n",
        "\n",
        "    # return a list containing the total rewards of all training episodes  \n",
        "    return train_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44z83bpLjYXD"
      },
      "source": [
        "### **Action model update**\n",
        "Implement the **simple_dqn_update** function to update action model weights using *gradient descent* algorithm given:\n",
        "- the action model (*dqn_action_model*);\n",
        "- the target model (*dqn_target_model*);\n",
        "- a mini-batch containing transitions $<s_i,a_i,r_{i+1},s_{i+1}>$ randomly selected from the replay memory (*mini_batch*); \n",
        "- the discount factor $\\gamma$ (gamma).\n",
        "\n",
        "The following parts need to be implemented:\n",
        "1. estimate $\\hat{Q}(s_{i+1},a)$ for all possible actions using the **predict** method of the target model with *new_state_batch* ($s_{i+1}$) as input parameter;\n",
        "2. estimate $Q(s_i,a)$ for all possible actions using the **predict** method of the action model with *state_batch* ($s_i$) as input parameter;\n",
        "3. update weights of the action model using the [**train_on_batch**](https://keras.io/api/models/model_training_apis/#trainonbatch-method) method with *state_batch* ($s_i$) and *predicted_state_q_values* ($y_i$) as *x* and *y* input parameters, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c4d_07hbT5k"
      },
      "outputs": [],
      "source": [
        "def simple_dqn_update(dqn_action_model, dqn_target_model, mini_batch, gamma):\n",
        "    # the transition mini-batch is divided into a mini-batch for each element of a transition\n",
        "    state_batch,action_batch, reward_batch, new_state_batch, done_batch = mini_batch\n",
        "\n",
        "    # 1. find the target model Q values for all possible actions given the new state batch\n",
        "    target_new_state_q_values = dqn_target_model.predict(new_state_batch)\n",
        "    \n",
        "    # 2. find the action model Q values for all possible actions given the current state batch\n",
        "    predicted_state_q_values = dqn_action_model.predict(state_batch)\n",
        "    \n",
        "    # estimate the target values y_i\n",
        "    # for the action we took, use the target model Q values  \n",
        "    # for other actions, use the action model Q values\n",
        "    # in this way, loss function will be 0 for other actions\n",
        "    for i, (a, r, new_state_q_values, done) in enumerate(zip(action_batch, reward_batch, target_new_state_q_values, done_batch)): \n",
        "        if not done:  \n",
        "          target_value = r + gamma * np.amax(new_state_q_values)\n",
        "        else:         \n",
        "          target_value = r\n",
        "        predicted_state_q_values[i][a] = target_value #y_i\n",
        "    \n",
        "    # 3. update weights of action model using the train_on_batch method \n",
        "    dqn_action_model.train_on_batch(state_batch, predicted_state_q_values)\n",
        "    \n",
        "    # return the updated action model\n",
        "    return dqn_action_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqK5u_MugIa4"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to start the training process by calling the **simple dqn_learning** function.\n",
        "\n",
        "The following parameters must be set before being passed to the function:\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum number of transitions stored into the replay memory (*replay_memory_max_size*);\n",
        "- the number of transitions stored into the replay memory before starting the training process (*replay_memory_init_size*);\n",
        "- the mini-batch size (*batch_size*);\n",
        "- the number of total steps executed between successive updates of the action model weights (*step_per_update*);\n",
        "- the number of total steps executed between successive replaces of the target model weights with the weights of the action model (*step_per_update_target_model*); \n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the discount factor $\\gamma$ (gamma);\n",
        "- the number of consecutive episodes to be considered in the calculation of the total reward moving average (*moving_avg_window_size*);\n",
        "- the minimum value of the total reward moving average to consider the task solved (*moving_avg_stop_thr*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNoeJ8_TgikQ"
      },
      "outputs": [],
      "source": [
        "episode_count = 500               # Total number of training episodes\n",
        "episode_max_steps = 400             # Maximum number of steps per episode\n",
        "\n",
        "replay_memory_max_size = 100000   # Maximum number of transitions stored into the replay memory\n",
        "replay_memory_init_size = 1000      # Maximum number of transitions stored into the replay memory\n",
        "batch_size = 64                   # Mini-batch size\n",
        "\n",
        "step_per_update = 4               # Number of total steps executed between successive updates of the action model weights\n",
        "step_per_update_target_model = 8    # Number of total steps executed between successive replaces of the target model weights\n",
        "\n",
        "max_epsilon = 1.0                   # Exploration probability at start\n",
        "min_epsilon = 0.01                  # Minimum exploration probability\n",
        "epsilon_decay = 0.0002              # Decay for exploration probability\n",
        "\n",
        "gamma = 0.99                      # Discount factor\n",
        "\n",
        "moving_avg_window_size = 20         # Number of consecutive episodes to be considered in the calculation of the total reward moving average\n",
        "moving_avg_stop_thr = 100           # Minimum value of the total reward moving average to consider the task solved\n",
        "\n",
        "train_start_time = time.time()\n",
        "train_rewards=simple_dqn_training(env,\n",
        "                                  simple_dqn_action_model,\n",
        "                                  simple_dqn_target_model,\n",
        "                                  episode_count,\n",
        "                                  episode_max_steps,\n",
        "                                  replay_memory_max_size,\n",
        "                                  replay_memory_init_size,\n",
        "                                  batch_size,\n",
        "                                  step_per_update,\n",
        "                                  step_per_update_target_model,\n",
        "                                  max_epsilon,\n",
        "                                  min_epsilon,\n",
        "                                  epsilon_decay,\n",
        "                                  gamma,\n",
        "                                  moving_avg_window_size,\n",
        "                                  moving_avg_stop_thr)\n",
        "\n",
        "train_finish_time = time.time()\n",
        "train_elapsed_time = train_finish_time - train_start_time\n",
        "train_avg_episode_time = train_elapsed_time / episode_count\n",
        "print(\"Train time: {:.1f}m [{:.1f}s]\".format(train_elapsed_time / 60.0, train_avg_episode_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5BZGXqQnYO3"
      },
      "source": [
        "### **Visualize the training process**\n",
        "It is important to observe how performance changes over time during the training process.\n",
        "\n",
        "The **plot_training_rewards** function defined above is used to draw in a graph the total reward and its moving average reached during the different episodes of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I60VmdG10X9"
      },
      "outputs": [],
      "source": [
        "plot_training_rewards(train_rewards, moving_avg_window_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_57XUhJ2ePR"
      },
      "source": [
        "## **Performance evaluation**\n",
        "The CartPole environment can be considered solved when the average total reward over 100 consecutive trials is greater than or equal to 195."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-8f12qB3Ni5"
      },
      "outputs": [],
      "source": [
        "episode_count = 5\n",
        "\n",
        "sum_episode_rewards = 0\n",
        "for episode in range(episode_count):\n",
        "  state = env.reset()\n",
        "\n",
        "  episode_reward = 0\n",
        "  done = False\n",
        "  step_count = 0\n",
        "  while not done:\n",
        "      q_values = simple_dqn_action_model.predict(state[np.newaxis])\n",
        "      action = np.argmax(q_values)\n",
        "      \n",
        "      new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      state = new_state\n",
        "      episode_reward += reward\n",
        "\n",
        "  sum_episode_rewards += episode_reward\n",
        "\n",
        "  print('Episode: {} Total reward: {}'.format(episode, episode_reward))\n",
        "\n",
        "print('Average total reward: {:.1f}'.format(sum_episode_rewards / episode_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o99SPQh5NNt"
      },
      "source": [
        "## **Single episode visualization**\n",
        "To show the CartPole agent in action on *Colab*, it is not possible to render each frame at real time.\n",
        "\n",
        "It is necessary to:\n",
        "1. execute an entire episode storing all episode frames;\n",
        "2. create an MP4 video with all frames;\n",
        "3. visualize the video.\n",
        "\n",
        "The following code executes an episode using the trained DQN network and store all frames into a list (*frames*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvKVS2z3OqhK"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "\n",
        "frames = []\n",
        "done = False\n",
        "episode_reward = 0\n",
        "while not done:\n",
        "    frame = env.render(mode = 'rgb_array')\n",
        "    frames.append(frame)\n",
        "    \n",
        "    q_values = simple_dqn_action_model.predict(state[np.newaxis])\n",
        "    action = np.argmax(q_values)\n",
        "    \n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    state = new_state\n",
        "\n",
        "    episode_reward += reward\n",
        "\n",
        "print('Total reward: ', episode_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vbC3ILJUzsK"
      },
      "source": [
        "Then the MP4 video file, created using the **create_mp4_video_from_frames** function, is included into an HTML video tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3PbO71QPuub"
      },
      "outputs": [],
      "source": [
        "vide_file_name = create_mp4_video_from_frames(frames, 30)\n",
        "\n",
        "mp4 = open(vide_file_name, 'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "02 - Reinforcement learning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
